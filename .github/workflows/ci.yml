name: FairClaimRCM CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly builds at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.10'
  NODE_VERSION: '18'

jobs:
  # Code Quality Checks
  lint-and-format:
    name: Code Quality & Formatting
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black flake8 mypy bandit safety
        pip install -r requirements.txt
    
    - name: Format check with Black
      run: |
        black --check --diff .
        
    - name: Lint with Flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type checking with MyPy
      run: |
        mypy api/ core/ --ignore-missing-imports
    
    - name: Security scan with Bandit
      run: |
        bandit -r api/ core/ -f json -o bandit-report.json
        bandit -r api/ core/
    
    - name: Dependency vulnerability scan
      run: |
        safety check --json --output safety-report.json
        safety check
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json

  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-${{ matrix.python-version }}-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-cov pytest-xdist pytest-mock
    
    - name: Run unit tests
      run: |
        pytest tests/ -m "unit" --cov=api --cov=core --cov-report=xml --cov-report=html -v
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: true
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          htmlcov/
          coverage.xml

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: fairclaimrcm_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-asyncio
    
    - name: Set up test database
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/fairclaimrcm_test
        REDIS_URL: redis://localhost:6379
      run: |
        python -c "
        import asyncio
        from api.models.database import Base, engine
        Base.metadata.create_all(bind=engine)
        "
    
    - name: Run integration tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/fairclaimrcm_test
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/ -m "integration" --maxfail=5 -v
    
    - name: Run API tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/fairclaimrcm_test
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/ -m "api" --maxfail=5 -v

  # Machine Learning Tests
  ml-tests:
    name: ML Component Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-ml-pip-${{ hashFiles('**/requirements.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
    
    - name: Download test data
      run: |
        mkdir -p data/test
        # In a real scenario, you'd download actual test datasets
        echo "Mock test data setup"
    
    - name: Run ML tests
      run: |
        pytest tests/ -m "ml" -v --tb=short
    
    - name: Run performance tests
      run: |
        pytest tests/ -m "slow" --maxfail=1 -v

  # Security Tests
  security-tests:
    name: Security & Vulnerability Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit safety semgrep
    
    - name: Run Bandit security scan
      run: |
        bandit -r api/ core/ -ll -f json -o bandit-results.json
    
    - name: Run Safety vulnerability scan
      run: |
        safety check --json --output safety-results.json
    
    - name: Run Semgrep security scan
      run: |
        semgrep --config=auto --json --output=semgrep-results.json api/ core/
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-scan-results
        path: |
          bandit-results.json
          safety-results.json
          semgrep-results.json

  # Docker Build Test
  docker-build:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      run: |
        docker build -t fairclaimrcm:test .
    
    - name: Test Docker image
      run: |
        docker run --rm --name fairclaimrcm-test -d -p 8000:8000 fairclaimrcm:test
        sleep 10
        curl -f http://localhost:8000/health || exit 1
        docker stop fairclaimrcm-test
    
    - name: Run container security scan
      run: |
        docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
          -v $(pwd):/root/.cache/ aquasec/trivy:latest image \
          --exit-code 0 --severity HIGH,CRITICAL fairclaimrcm:test

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust
    
    - name: Start application
      run: |
        uvicorn api.main:app --host 0.0.0.0 --port 8000 &
        sleep 10
      env:
        DATABASE_URL: sqlite:///./test_perf.db
    
    - name: Run performance tests
      run: |
        locust -f tests/performance/locustfile.py --headless --users 10 --spawn-rate 2 --run-time 60s --host http://localhost:8000

  # Deployment Tests
  deployment-tests:
    name: Deployment Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, security-tests]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Test deployment configuration
      run: |
        python -c "
        from api.main import app
        from core.config import settings
        print('✓ Application imports successfully')
        print(f'✓ Database URL configured: {bool(settings.DATABASE_URL)}')
        print(f'✓ Environment: {settings.ENVIRONMENT}')
        "
    
    - name: Validate database migrations
      run: |
        # Test that migrations can be applied
        python -c "
        from api.models.database import Base, engine
        try:
            Base.metadata.create_all(bind=engine)
            print('✓ Database schema creation successful')
        except Exception as e:
            print(f'✗ Database schema creation failed: {e}')
            exit(1)
        "

  # Build Summary
  build-summary:
    name: Build Summary
    runs-on: ubuntu-latest
    needs: [lint-and-format, unit-tests, integration-tests, ml-tests, security-tests, docker-build]
    if: always()
    
    steps:
    - name: Build Summary
      run: |
        echo "## Build Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Component | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|-----------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Code Quality | ${{ needs.lint-and-format.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| ML Tests | ${{ needs.ml-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Tests | ${{ needs.security-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Docker Build | ${{ needs.docker-build.result }} |" >> $GITHUB_STEP_SUMMARY
